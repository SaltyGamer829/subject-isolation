{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Preprocess image (resize, normalize)\n",
    "IMG_SIZE = (256, 256)\n",
    "NUM_CLASSES = 21 # 20 classes + the background\n",
    "\n",
    "# Transformation class\n",
    "class VOCTransforms:\n",
    "    def __init__(self, img_size, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToImage(),\n",
    "            transforms.ToDtype(torch.float32, scale=True),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "        self.target_transform = transforms.Compose([\n",
    "            transforms.Resize(img_size, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.ToImage(),\n",
    "            transforms.ToDtype(torch.long, scale=False),\n",
    "        ])\n",
    "    def __call__(self, img, target):\n",
    "        image = self.transform(img)\n",
    "        target = self.target_transform(target)\n",
    "        target = target.squeeze(0)\n",
    "        return image, target\n",
    "\n",
    "# Load dataset, image and segmentation mask\n",
    "\n",
    "voc_transforms = VOCTransforms(IMG_SIZE)\n",
    "\n",
    "DATA_ROOT = '/Users/ianyoo/Documents/School/ECS 171/Final Project/subject-isolation/data'\n",
    "\n",
    "train_dataset = VOCSegmentation(\n",
    "    root=DATA_ROOT,\n",
    "    year='2012',\n",
    "    image_set='train',\n",
    "    download=False,\n",
    "    transforms=voc_transforms\n",
    ")\n",
    "\n",
    "val_dataset = VOCSegmentation(\n",
    "    root=DATA_ROOT,\n",
    "    year='2012',\n",
    "    image_set='val',\n",
    "    download=False,\n",
    "    transforms=voc_transforms\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4 \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for double convolution\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels), # Add batch normalization\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Second convolution\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv\n",
    "\n",
    "# Downscaling with maxpool then double convolution\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv\n",
    "\n",
    "# Upscaling then double conv\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # if bilinear use normal convolutions to reduce channel num\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1) # Concatenate along channel dimension (SKIP CONNECTION)\n",
    "        return self.conv(x)\n",
    "\n",
    "# Final layer class to map to output classes (21)\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Define U-Net model:\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNET, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        # Encoder path (downsampling):\n",
    "            # conv -> relu -> conv -> relu -> maxpool\n",
    "            # repeat, doubling channels\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor) # Bottom of the U\n",
    "\n",
    "        # Decoder path (upsampling):\n",
    "            # upsample -> concat with encoder feature map -> convs\n",
    "            # repeat, halving channels\n",
    "        # Note: In 'Up', in_channels is the sum from skip + upsample\n",
    "        self.up1 = Up(1024, 512, bilinear)\n",
    "        self.up2 = Up(512, 256, bilinear)\n",
    "        self.up3 = Up(256, 128, bilinear)\n",
    "        self.up4 = Up(128, 64 * factor, bilinear) # Adjusted for concat\n",
    "        \n",
    "        # Final 1x1 conv -> class scores for each pixel\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x): # flow of input\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3) \n",
    "        x = self.up3(x, x2) \n",
    "        x = self.up4(x, x1) \n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained model to predict segmentation masks\n",
    "device = torch.device('cpu')\n",
    "model = UNET(n_channels=3, n_classes=NUM_CLASSES).to(device)\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 25\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255) # loss func\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "print(\"Train start\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'unet_voc_final.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
