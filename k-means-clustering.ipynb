{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "# Reshape image to 2D array of pixels (each pixel = [R, G, B])\n",
    "\n",
    "# Choose number of clusters (k)\n",
    "\n",
    "# Initialize k cluster centers randomly\n",
    "\n",
    "# Repeat until convergence or max iterations:\n",
    "    # Assign each pixel to the nearest cluster center\n",
    "    # Update cluster centers as the mean of assigned pixels\n",
    "\n",
    "# Replace each pixel with its cluster center color\n",
    "\n",
    "# Reshape back to original image shape and display/save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.7.0-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from torch) (78.1.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from torchvision) (2.2.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.7.0-cp313-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Downloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m847.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.7.0-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.5.1 mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import PILToTensor, ToTensor\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#transformation class\n",
    "class ToTensorPair:\n",
    "    def __call__(self, image, target):\n",
    "        image = ToTensor()(image)\n",
    "        target = torch.from_numpy(np.array(target)).long()\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "data_root = '/Users/sarayumum/Desktop/ucdavis/ecs171/subject-isolation/dataset'\n",
    "transform = ToTensorPair()\n",
    "train_dataset = VOCSegmentation(\n",
    "    root=data_root,\n",
    "    year=\"2012\",\n",
    "    image_set=\"train\",\n",
    "    download=True, \n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "val_dataset = VOCSegmentation(\n",
    "    root=data_root,\n",
    "    year=\"2012\",\n",
    "    image_set=\"val\",\n",
    "    download=True, \n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Data Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 281, 500])\n",
      "Mask shape: torch.Size([281, 500])\n"
     ]
    }
   ],
   "source": [
    "image, mask = train_dataset[0]\n",
    "print(\"Image shape:\", image.shape) \n",
    "print(\"Mask shape:\", mask.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: tensor([[[0.5137, 0.7569, 0.8157],\n",
      "         [0.5137, 0.7569, 0.8157],\n",
      "         [0.5137, 0.7569, 0.8157],\n",
      "         ...,\n",
      "         [0.7137, 0.9098, 0.9451],\n",
      "         [0.7137, 0.9098, 0.9451],\n",
      "         [0.7137, 0.9098, 0.9451]],\n",
      "\n",
      "        [[0.5176, 0.7608, 0.8196],\n",
      "         [0.5176, 0.7608, 0.8196],\n",
      "         [0.5216, 0.7647, 0.8235],\n",
      "         ...,\n",
      "         [0.7176, 0.9137, 0.9490],\n",
      "         [0.7176, 0.9137, 0.9490],\n",
      "         [0.7137, 0.9098, 0.9451]],\n",
      "\n",
      "        [[0.5216, 0.7647, 0.8235],\n",
      "         [0.5216, 0.7647, 0.8235],\n",
      "         [0.5255, 0.7686, 0.8275],\n",
      "         ...,\n",
      "         [0.7176, 0.9137, 0.9490],\n",
      "         [0.7176, 0.9137, 0.9490],\n",
      "         [0.7137, 0.9098, 0.9451]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0235, 0.0588, 0.0549],\n",
      "         [0.0196, 0.0549, 0.0510],\n",
      "         [0.0275, 0.0510, 0.0510],\n",
      "         ...,\n",
      "         [0.0627, 0.1333, 0.1490],\n",
      "         [0.0627, 0.1333, 0.1490],\n",
      "         [0.0627, 0.1333, 0.1490]],\n",
      "\n",
      "        [[0.0196, 0.0549, 0.0510],\n",
      "         [0.0353, 0.0706, 0.0667],\n",
      "         [0.0471, 0.0667, 0.0784],\n",
      "         ...,\n",
      "         [0.0627, 0.1333, 0.1490],\n",
      "         [0.0667, 0.1373, 0.1529],\n",
      "         [0.0667, 0.1373, 0.1529]],\n",
      "\n",
      "        [[0.0196, 0.0588, 0.0549],\n",
      "         [0.0235, 0.0627, 0.0588],\n",
      "         [0.0392, 0.0706, 0.0784],\n",
      "         ...,\n",
      "         [0.0627, 0.1294, 0.1569],\n",
      "         [0.0471, 0.1137, 0.1412],\n",
      "         [0.0667, 0.1333, 0.1608]]])\n"
     ]
    }
   ],
   "source": [
    "image_perumated = image.permute(1, 2, 0)\n",
    "print(\"Image shape:\", image_perumated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: tensor([[0.5137, 0.7569, 0.8157],\n",
      "        [0.5137, 0.7569, 0.8157],\n",
      "        [0.5137, 0.7569, 0.8157],\n",
      "        ...,\n",
      "        [0.0627, 0.1294, 0.1569],\n",
      "        [0.0471, 0.1137, 0.1412],\n",
      "        [0.0667, 0.1333, 0.1608]])\n"
     ]
    }
   ],
   "source": [
    "image_reshaped = image_perumated.reshape(-1, 3)\n",
    "print(\"Image shape:\", image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6431, 0.6471, 0.5765],\n",
      "        [0.6667, 0.6706, 0.6000],\n",
      "        [0.6706, 0.6824, 0.6000],\n",
      "        ...,\n",
      "        [0.2863, 0.3059, 0.3216],\n",
      "        [0.2784, 0.2980, 0.3216],\n",
      "        [0.2667, 0.2863, 0.3098]])\n"
     ]
    }
   ],
   "source": [
    "train_pixels = []\n",
    "val_pixels = []\n",
    "\n",
    "def pixels(dataset):\n",
    "    pixels = []\n",
    "    for image, _ in dataset:\n",
    "        image_perumated = image.permute(1, 2, 0)\n",
    "        image_reshaped = image_perumated.reshape(-1, 3)\n",
    "        pixels.append(image_reshaped)\n",
    "    return pixels\n",
    "\n",
    "train_pixels = pixels(train_dataset)\n",
    "val_pixels = pixels(val_dataset)\n",
    "print(train_pixels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140500, 3])\n"
     ]
    }
   ],
   "source": [
    "print(train_pixels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8980392  0.74509805 0.48235294]\n",
      " [0.04313726 0.01176471 0.00392157]\n",
      " [0.47843137 0.47058824 0.16862746]\n",
      " [0.94509804 0.9607843  0.972549  ]\n",
      " [1.         0.92941177 0.7921569 ]\n",
      " [0.3647059  0.47058824 0.6784314 ]\n",
      " [0.9843137  0.99215686 0.9882353 ]\n",
      " [0.0627451  0.03921569 0.04705882]\n",
      " [0.6431373  0.7176471  0.78431374]\n",
      " [0.14117648 0.12941177 0.10980392]\n",
      " [0.06666667 0.08235294 0.07843138]\n",
      " [0.65882355 0.67058825 0.44705883]\n",
      " [0.85490197 0.827451   0.654902  ]\n",
      " [0.69411767 0.74509805 0.8117647 ]\n",
      " [0.627451   0.6745098  0.7294118 ]\n",
      " [0.3254902  0.53333336 0.74509805]\n",
      " [0.22745098 0.24313726 0.25490198]\n",
      " [0.70980394 0.7254902  0.49411765]\n",
      " [0.24705882 0.3764706  0.34117648]\n",
      " [0.43137255 0.41568628 0.41960785]\n",
      " [0.07450981 0.1254902  0.15686275]]\n",
      "[[0.5137255  0.75686276 0.8156863 ]\n",
      " [0.5137255  0.75686276 0.8156863 ]\n",
      " [0.5137255  0.75686276 0.8156863 ]\n",
      " ...\n",
      " [0.12941177 0.30588236 0.3254902 ]\n",
      " [0.04705882 0.34117648 0.3647059 ]\n",
      " [0.14901961 0.29803923 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "#concatenate the train and val pixels\n",
    "total_train_pixels = torch.cat(train_pixels, dim  = 0)\n",
    "total_train_pixels = np.float32(total_train_pixels)\n",
    "\n",
    "classes = 21 #number of classes + background\n",
    "num_train_pixels = len(total_train_pixels)\n",
    "indices = np.random.choice(num_train_pixels, classes, replace=False)\n",
    "centers = total_train_pixels[indices]\n",
    "print(centers)\n",
    "print(total_train_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans training complete.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Buffer dtype mismatch, expected 'const float' but got 'double'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m     21\u001b[39m     image_tensor, _ = val_dataset[i]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     segmented = \u001b[43msegment_image_with_kmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Show original and segmented side-by-side\u001b[39;00m\n\u001b[32m     25\u001b[39m     plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m4\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36msegment_image_with_kmeans\u001b[39m\u001b[34m(image_tensor, kmeans_model)\u001b[39m\n\u001b[32m     12\u001b[39m h, w = image_tensor.shape[\u001b[32m1\u001b[39m], image_tensor.shape[\u001b[32m2\u001b[39m]\n\u001b[32m     13\u001b[39m image_np = image_tensor.permute(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m).reshape(-\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m).numpy().astype(np.float32)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m labels = \u001b[43mkmeans_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m segmented_pixels = kmeans_model.cluster_centers_[labels]\n\u001b[32m     16\u001b[39m segmented_image = segmented_pixels.reshape(h, w, \u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/sklearn/cluster/_kmeans.py:1090\u001b[39m, in \u001b[36m_BaseKMeans.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# sample weights are not used by predict but cython helpers expect an array\u001b[39;00m\n\u001b[32m   1088\u001b[39m sample_weight = np.ones(X.shape[\u001b[32m0\u001b[39m], dtype=X.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m labels = \u001b[43m_labels_inertia_threadpool_limit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcluster_centers_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_n_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_inertia\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/sklearn/utils/parallel.py:165\u001b[39m, in \u001b[36m_threadpool_controller_decorator.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m controller = _get_threadpool_controller()\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m controller.limit(limits=limits, user_api=user_api):\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/sklearn/cluster/_kmeans.py:806\u001b[39m, in \u001b[36m_labels_inertia\u001b[39m\u001b[34m(X, sample_weight, centers, n_threads, return_inertia)\u001b[39m\n\u001b[32m    803\u001b[39m     _labels = lloyd_iter_chunked_dense\n\u001b[32m    804\u001b[39m     _inertia = _inertia_dense\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m \u001b[43m_labels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenters_new\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_in_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenter_shift\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter_shift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_centers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_inertia:\n\u001b[32m    819\u001b[39m     inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_k_means_lloyd.pyx:26\u001b[39m, in \u001b[36msklearn.cluster._k_means_lloyd.lloyd_iter_chunked_dense\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Buffer dtype mismatch, expected 'const float' but got 'double'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "pixels = np.uint8(total_train_pixels * 255)\n",
    "k = 21  \n",
    "kmeans = MiniBatchKMeans(n_clusters=21, batch_size=10000, random_state=0)\n",
    "kmeans.fit(pixels)\n",
    "print(\"KMeans training complete.\")\n",
    "\n",
    "# Segment a sample image using fitted KMeans\n",
    "def segment_image_with_kmeans(image_tensor, kmeans_model):\n",
    "    h, w = image_tensor.shape[1], image_tensor.shape[2]\n",
    "    image_np = image_tensor.permute(1, 2, 0).reshape(-1, 3).numpy().astype(np.float32)\n",
    "    labels = kmeans_model.predict(image_np)\n",
    "    segmented_pixels = kmeans_model.cluster_centers_[labels]\n",
    "    segmented_image = segmented_pixels.reshape(h, w, 3)\n",
    "    return segmented_image\n",
    "\n",
    "# Visualize results for first few val images\n",
    "for i in range(3):\n",
    "    image_tensor, _ = val_dataset[i]\n",
    "    segmented = segment_image_with_kmeans(image_tensor, kmeans)\n",
    "\n",
    "    # Show original and segmented side-by-side\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_tensor.permute(1, 2, 0).numpy())\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(segmented)\n",
    "    plt.title(\"Segmented with KMeans\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
